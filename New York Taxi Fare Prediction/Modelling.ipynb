{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import warnings\n",
    "from math import sin, cos, sqrt, atan2, radians,asin\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training Data (3000000, 8)\n",
      "Shape of Testing Data (9914, 7)\n"
     ]
    }
   ],
   "source": [
    "train=pd.read_csv(\"train.csv\",nrows=3000000)\n",
    "print(\"Shape of Training Data\",train.shape)\n",
    "test=pd.read_csv(\"test.csv\")\n",
    "print(\"Shape of Testing Data\", test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data \n",
    "1. Remove fare amount < 0 \n",
    "2. Passenger count > 7 remove from train data\n",
    "3. Use only the fields that are present to create Baseline Model - have only date features. Rest of the features we will add later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeDays(day_of_week):\n",
    "    day_dict={'Sunday':0,'Monday':1,'Tuesday':2,'Wednesday':3,'Thursday':4,'Friday':5,'Saturday':6}\n",
    "    return day_dict[day_of_week]\n",
    "def clean_data(data):\n",
    "    boundary={'min_lng':-74.263242,\n",
    "              'min_lat':40.573143,\n",
    "              'max_lng':-72.986532, \n",
    "              'max_lat':41.709555}\n",
    "    \n",
    "    data['pickup_datetime']=pd.to_datetime(data['pickup_datetime'],format='%Y-%m-%d %H:%M:%S UTC')\n",
    "    data['pickup_day']=data['pickup_datetime'].apply(lambda x:x.day)\n",
    "    data['pickup_hour']=data['pickup_datetime'].apply(lambda x:x.hour)\n",
    "    data['pickup_day_of_week']=data['pickup_datetime'].apply(lambda x:calendar.day_name[x.weekday()])\n",
    "    data['pickup_month']=data['pickup_datetime'].apply(lambda x:x.month)\n",
    "    data['pickup_year']=data['pickup_datetime'].apply(lambda x:x.year)\n",
    "    if 'fare_amount' in data.columns:\n",
    "        data=data[data['fare_amount']>=0]\n",
    "        data.loc[~((data.pickup_longitude >= boundary['min_lng'] ) & (data.pickup_longitude <= boundary['max_lng']) &\n",
    "            (data.pickup_latitude >= boundary['min_lat']) & (data.pickup_latitude <= boundary['max_lat']) &\n",
    "            (data.dropoff_longitude >= boundary['min_lng']) & (data.dropoff_longitude <= boundary['max_lng']) &\n",
    "            (data.dropoff_latitude >=boundary['min_lat']) & (data.dropoff_latitude <= boundary['max_lat'])),'is_outlier_loc']=1\n",
    "        data.loc[((data.pickup_longitude >= boundary['min_lng'] ) & (data.pickup_longitude <= boundary['max_lng']) &\n",
    "            (data.pickup_latitude >= boundary['min_lat']) & (data.pickup_latitude <= boundary['max_lat']) &\n",
    "            (data.dropoff_longitude >= boundary['min_lng']) & (data.dropoff_longitude <= boundary['max_lng']) &\n",
    "            (data.dropoff_latitude >=boundary['min_lat']) & (data.dropoff_latitude <= boundary['max_lat'])),'is_outlier_loc']=0\n",
    "\n",
    "    #print(\"Outlier vs Non Outlier Counts\")\n",
    "    #print(data['is_outlier_loc'].value_counts())\n",
    "\n",
    "    # Let us drop rows, where location is outlier\n",
    "        data=data.loc[data['is_outlier_loc']==0]\n",
    "        data.drop(['is_outlier_loc'],axis=1,inplace=True)\n",
    "    \n",
    "    data=data[data['passenger_count']<=8]\n",
    "    data['pickup_day_of_week']=data['pickup_day_of_week'].apply(lambda x:encodeDays(x))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training Data after cleaning  (2935680, 13)\n",
      "Shape of Testing Data after cleaning (9914, 12)\n"
     ]
    }
   ],
   "source": [
    "train=clean_data(train)\n",
    "test=clean_data(test)\n",
    "print(\"Shape of Training Data after cleaning \",train.shape)\n",
    "print(\"Shape of Testing Data after cleaning\", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to process data for Modelling\n",
    "This step includes:\n",
    "1. Dropping unwanted columns from the data\n",
    "2. One Hot Encoding of categorical variables\n",
    "3. Dividing training data into train and validation data sets\n",
    "    1. features and target varible must be seperated\n",
    "    2. split ratio must be passed as an argument "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDataForModelling(data,target,drop_cols,is_train=True,split=0.25):\n",
    "    data_1=data.drop(drop_cols,axis=1)\n",
    "    # One hot Encoding\n",
    "    data_1=pd.get_dummies(data_1)\n",
    "    if is_train==True:\n",
    "        X=data_1.drop([target],axis=1)\n",
    "        y=data_1[target]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=split,random_state=123)\n",
    "        \n",
    "        print(\"Shape of Training Features\",X_train.shape)\n",
    "        print(\"Shape of Validation Features \",X_test.shape)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    else:\n",
    "        print (\"Shape of Test Data\",data_1.shape)\n",
    "        return data_1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training Features (2348544, 10)\n",
      "Shape of Validation Features  (587136, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test=processDataForModelling(train,'fare_amount',drop_cols=['key','pickup_datetime'],is_train=True,split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Test Data (9914, 10)\n"
     ]
    }
   ],
   "source": [
    "test_data=processDataForModelling(test,'fare_amount',drop_cols=['key','pickup_datetime'],is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Baseline Model and Identifying a good ML algorithm for this problem\n",
    "The metric used in this problem is RMSE. \n",
    "We will try three models - Linear Regression, Random Forest and XGBoost and see which model performs better.\n",
    "We will use the best model among the three to further tune and apply feature Engineering\n",
    "\n",
    "For Baseline, we will predict the average fare amount and check the RMSE on validation data. Any model, should be able to beat this simple benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.31"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_fare=round(np.mean(y_train),2)\n",
    "avg_fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basline RMSE of Validation data : 9.707556295335856\n"
     ]
    }
   ],
   "source": [
    "baseline_pred=np.repeat(avg_fare,y_test.shape[0])\n",
    "baseline_rmse=np.sqrt(mean_squared_error(baseline_pred, y_test))\n",
    "print(\"Basline RMSE of Validation data :\",baseline_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build a Linear Regression Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Linear Regression is  8.239673655728213\n"
     ]
    }
   ],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X_train,y_train)\n",
    "y_pred=np.round(lm.predict(X_test),2)\n",
    "lm_rmse=np.sqrt(mean_squared_error(y_pred, y_test))\n",
    "print(\"RMSE for Linear Regression is \",lm_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression model performed better than than the Baseline Mode. Let us create a Random Forest Model and see how it performs. Reason for failure of logistic regression model, is that it tries to fit a linear line between the variables and the targer. But, as we saw in the Exploratory analysis phase this is not true. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build a Random Forest Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "           oob_score=False, random_state=883, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators = 100, random_state = 883,n_jobs=-1)\n",
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Random Forest is  3.722760643979709\n"
     ]
    }
   ],
   "source": [
    "rf_pred= rf.predict(X_test)\n",
    "rf_rmse=np.sqrt(mean_squared_error(rf_pred, y_test))\n",
    "print(\"RMSE for Random Forest is \",rf_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest has reduced the RMSE considerably as compared to Linear Regression. Random Forest works on the principle of Bagging\n",
    "The idea behind this is very intutive- when we want to make a decision about a field we do not know about we take advice from a people and then we decide based on the majority opinion.\n",
    "This is the idea behind Random Forest - multiple decision trees are created and output from each of the trees are averaged to predict the value.\n",
    "Random Forest are not susceptible to overfitting and since in Random Forest, each tree is trained independently of the other, it is more robust.\n",
    "\n",
    "The next type of algorithm we will see is also another Tree based Ensemble algorithm - but it follows concept of Boosting. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building LightGBM Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=lgb.Dataset(X_train,label=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'num_leaves':31, 'num_trees':5000, 'objective':'regression'}\n",
    "param['metric'] = 'l2_root'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\tcv_agg's rmse: 5.06279 + 0.05548\n",
      "[40]\tcv_agg's rmse: 4.50898 + 0.0472365\n",
      "[60]\tcv_agg's rmse: 4.30951 + 0.0631628\n",
      "[80]\tcv_agg's rmse: 4.21842 + 0.0587082\n",
      "[100]\tcv_agg's rmse: 4.15903 + 0.0561534\n",
      "[120]\tcv_agg's rmse: 4.11538 + 0.0551337\n",
      "[140]\tcv_agg's rmse: 4.08347 + 0.0569919\n",
      "[160]\tcv_agg's rmse: 4.05952 + 0.0571602\n",
      "[180]\tcv_agg's rmse: 4.0394 + 0.0586051\n",
      "[200]\tcv_agg's rmse: 4.02268 + 0.0549178\n",
      "[220]\tcv_agg's rmse: 4.00745 + 0.0558276\n",
      "[240]\tcv_agg's rmse: 3.99439 + 0.0557403\n",
      "[260]\tcv_agg's rmse: 3.98173 + 0.0563307\n",
      "[280]\tcv_agg's rmse: 3.96865 + 0.0567871\n",
      "[300]\tcv_agg's rmse: 3.95948 + 0.0574369\n",
      "[320]\tcv_agg's rmse: 3.95052 + 0.0568952\n",
      "[340]\tcv_agg's rmse: 3.94325 + 0.0560929\n",
      "[360]\tcv_agg's rmse: 3.93594 + 0.056963\n",
      "[380]\tcv_agg's rmse: 3.9317 + 0.0577162\n",
      "[400]\tcv_agg's rmse: 3.92711 + 0.0572581\n",
      "[420]\tcv_agg's rmse: 3.921 + 0.0572053\n",
      "[440]\tcv_agg's rmse: 3.91641 + 0.0573622\n",
      "[460]\tcv_agg's rmse: 3.91227 + 0.0583486\n",
      "[480]\tcv_agg's rmse: 3.90848 + 0.0594168\n",
      "[500]\tcv_agg's rmse: 3.90463 + 0.0585824\n",
      "[520]\tcv_agg's rmse: 3.90127 + 0.0582272\n",
      "[540]\tcv_agg's rmse: 3.89902 + 0.0574853\n",
      "[560]\tcv_agg's rmse: 3.89534 + 0.0563872\n",
      "[580]\tcv_agg's rmse: 3.89201 + 0.0559782\n",
      "[600]\tcv_agg's rmse: 3.88935 + 0.0562413\n",
      "[620]\tcv_agg's rmse: 3.8873 + 0.0569952\n",
      "[640]\tcv_agg's rmse: 3.88488 + 0.0575079\n",
      "[660]\tcv_agg's rmse: 3.88272 + 0.0575091\n",
      "[680]\tcv_agg's rmse: 3.88092 + 0.0577236\n",
      "[700]\tcv_agg's rmse: 3.87851 + 0.057473\n",
      "[720]\tcv_agg's rmse: 3.87658 + 0.0566493\n",
      "[740]\tcv_agg's rmse: 3.87448 + 0.0561221\n",
      "[760]\tcv_agg's rmse: 3.87288 + 0.0558824\n",
      "[780]\tcv_agg's rmse: 3.87102 + 0.055542\n",
      "[800]\tcv_agg's rmse: 3.87007 + 0.0556144\n",
      "[820]\tcv_agg's rmse: 3.86904 + 0.0559794\n",
      "[840]\tcv_agg's rmse: 3.86755 + 0.0559521\n",
      "[860]\tcv_agg's rmse: 3.86646 + 0.0559041\n",
      "[880]\tcv_agg's rmse: 3.86487 + 0.055873\n",
      "[900]\tcv_agg's rmse: 3.86361 + 0.0566979\n",
      "[920]\tcv_agg's rmse: 3.86244 + 0.0568587\n",
      "[940]\tcv_agg's rmse: 3.86169 + 0.056678\n",
      "[960]\tcv_agg's rmse: 3.86101 + 0.056158\n",
      "[980]\tcv_agg's rmse: 3.86005 + 0.0558139\n",
      "[1000]\tcv_agg's rmse: 3.85886 + 0.0564037\n",
      "[1020]\tcv_agg's rmse: 3.85821 + 0.0566053\n",
      "[1040]\tcv_agg's rmse: 3.85704 + 0.0572947\n",
      "[1060]\tcv_agg's rmse: 3.85637 + 0.0574212\n",
      "[1080]\tcv_agg's rmse: 3.85595 + 0.0575894\n",
      "[1100]\tcv_agg's rmse: 3.85515 + 0.0576978\n",
      "[1120]\tcv_agg's rmse: 3.85444 + 0.0577136\n",
      "[1140]\tcv_agg's rmse: 3.8537 + 0.0577099\n",
      "[1160]\tcv_agg's rmse: 3.85325 + 0.0578788\n",
      "[1180]\tcv_agg's rmse: 3.85264 + 0.0582115\n",
      "[1200]\tcv_agg's rmse: 3.85214 + 0.0583684\n",
      "[1220]\tcv_agg's rmse: 3.85133 + 0.0586965\n",
      "[1240]\tcv_agg's rmse: 3.85107 + 0.0588698\n",
      "[1260]\tcv_agg's rmse: 3.8505 + 0.0591249\n",
      "[1280]\tcv_agg's rmse: 3.84995 + 0.0589086\n",
      "[1300]\tcv_agg's rmse: 3.84944 + 0.0587883\n",
      "[1320]\tcv_agg's rmse: 3.84889 + 0.0588022\n",
      "[1340]\tcv_agg's rmse: 3.84797 + 0.058595\n",
      "[1360]\tcv_agg's rmse: 3.84748 + 0.0588378\n",
      "[1380]\tcv_agg's rmse: 3.84697 + 0.0589701\n",
      "[1400]\tcv_agg's rmse: 3.84637 + 0.0592595\n",
      "[1420]\tcv_agg's rmse: 3.84599 + 0.0592941\n",
      "[1440]\tcv_agg's rmse: 3.84563 + 0.059031\n",
      "[1460]\tcv_agg's rmse: 3.845 + 0.0591092\n",
      "[1480]\tcv_agg's rmse: 3.84484 + 0.0593245\n",
      "[1500]\tcv_agg's rmse: 3.84455 + 0.0594181\n",
      "[1520]\tcv_agg's rmse: 3.84438 + 0.0590958\n",
      "[1540]\tcv_agg's rmse: 3.84367 + 0.0593565\n",
      "[1560]\tcv_agg's rmse: 3.84336 + 0.0595028\n",
      "[1580]\tcv_agg's rmse: 3.84354 + 0.0595959\n"
     ]
    }
   ],
   "source": [
    "num_round=5000\n",
    "cv_results = lgb.cv(param, train_data, num_boost_round=num_round, nfold=5,verbose_eval=20, early_stopping_rounds=20,stratified=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best num_boost_round: 1560\n"
     ]
    }
   ],
   "source": [
    "print('Best num_boost_round:', len(cv_results['rmse-mean']))\n",
    "#lgb_pred = lgb_bst.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_bst=lgb.train(param,train_data,len(cv_results['rmse-mean']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Light GBM is  3.8028190168518723\n"
     ]
    }
   ],
   "source": [
    "lgb_pred = lgb_bst.predict(X_test)\n",
    "lgb_rmse=np.sqrt(mean_squared_error(lgb_pred, y_test))\n",
    "print(\"RMSE for Light GBM is \",lgb_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LighGBM was much quicker than RF and the RMSE is quiet close to RF too. Tuning this can get better results. For the rest of this excercise we will go with LIGHT GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_airports={'JFK':{'min_lng':-73.8352,\n",
    "     'min_lat':40.6195,\n",
    "     'max_lng':-73.7401, \n",
    "     'max_lat':40.6659},\n",
    "              \n",
    "    'EWR':{'min_lng':-74.1925,\n",
    "            'min_lat':40.6700, \n",
    "            'max_lng':-74.1531, \n",
    "            'max_lat':40.7081\n",
    "\n",
    "        },\n",
    "    'LaGuardia':{'min_lng':-73.8895, \n",
    "                  'min_lat':40.7664, \n",
    "                  'max_lng':-73.8550, \n",
    "                  'max_lat':40.7931\n",
    "        \n",
    "    }\n",
    "    \n",
    "}\n",
    "def isAirport(latitude,longitude,airport_name='JFK'):\n",
    "    \n",
    "    if latitude>=nyc_airports[airport_name]['min_lat'] and latitude<=nyc_airports[airport_name]['max_lat'] and longitude>=nyc_airports[airport_name]['min_lng'] and longitude<=nyc_airports[airport_name]['max_lng']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_boroughs={\n",
    "    'manhattan':{\n",
    "        'min_lng':-74.0479,\n",
    "        'min_lat':40.6829,\n",
    "        'max_lng':-73.9067,\n",
    "        'max_lat':40.8820\n",
    "    },\n",
    "    \n",
    "    'queens':{\n",
    "        'min_lng':-73.9630,\n",
    "        'min_lat':40.5431,\n",
    "        'max_lng':-73.7004,\n",
    "        'max_lat':40.8007\n",
    "\n",
    "    },\n",
    "\n",
    "    'brooklyn':{\n",
    "        'min_lng':-74.0421,\n",
    "        'min_lat':40.5707,\n",
    "        'max_lng':-73.8334,\n",
    "        'max_lat':40.7395\n",
    "\n",
    "    },\n",
    "\n",
    "    'bronx':{\n",
    "        'min_lng':-73.9339,\n",
    "        'min_lat':40.7855,\n",
    "        'max_lng':-73.7654,\n",
    "        'max_lat':40.9176\n",
    "\n",
    "    },\n",
    "\n",
    "    'staten_island':{\n",
    "        'min_lng':-74.2558,\n",
    "        'min_lat':40.4960,\n",
    "        'max_lng':-74.0522,\n",
    "        'max_lat':40.6490\n",
    "\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBorough(lat,lng):\n",
    "    \n",
    "    locs=nyc_boroughs.keys()\n",
    "    for loc in locs:\n",
    "        if lat>=nyc_boroughs[loc]['min_lat'] and lat<=nyc_boroughs[loc]['max_lat'] and lng>=nyc_boroughs[loc]['min_lng'] and lng<=nyc_boroughs[loc]['max_lng']:\n",
    "            return loc\n",
    "    return 'others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_manhattan_boundary={'min_lng': -74.0194,\n",
    "                          'min_lat':40.6997,\n",
    "                          'max_lng':-73.9716,\n",
    "                          'max_lat':40.7427}\n",
    "\n",
    "def isLowerManhattan(lat,lng):\n",
    "    if lat>=lower_manhattan_boundary['min_lat'] and lat<=lower_manhattan_boundary['max_lat'] and lng>=lower_manhattan_boundary['min_lng'] and lng<=lower_manhattan_boundary['max_lng']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['is_pickup_lower_manhattan']=train.apply(lambda row:isLowerManhattan(row['pickup_latitude'],row['pickup_longitude']),axis=1)\n",
    "train['is_dropoff_lower_manhattan']=train.apply(lambda row:isLowerManhattan(row['dropoff_latitude'],row['dropoff_longitude']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['is_pickup_lower_manhattan']=test.apply(lambda row:isLowerManhattan(row['pickup_latitude'],row['pickup_longitude']),axis=1)\n",
    "test['is_dropoff_lower_manhattan']=test.apply(lambda row:isLowerManhattan(row['dropoff_latitude'],row['dropoff_longitude']),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(lat1,lon1,lat2,lon2):\n",
    "    p = 0.017453292519943295 # Pi/180\n",
    "    a = 0.5 - np.cos((lat2 - lat1) * p)/2 + np.cos(lat1 * p) * np.cos(lat2 * p) * (1 - np.cos((lon2 - lon1) * p)) / 2\n",
    "    return 0.6213712 * 12742 * np.arcsin(np.sqrt(a))\n",
    "train['trip_distance']=train.apply(lambda row:distance(row['pickup_latitude'],row['dropoff_latitude'],row['pickup_longitude'],row['dropoff_longitude']),axis=1)\n",
    "test['trip_distance']=test.apply(lambda row:distance(row['pickup_latitude'],row['dropoff_latitude'],row['pickup_longitude'],row['dropoff_longitude']),axis=1)\n",
    "\n",
    "lgr=(-73.8733, 40.7746)\n",
    "jfk=(-73.7900, 40.6437)\n",
    "ewr=(-74.1843, 40.6924)\n",
    "\n",
    "test['pickup_distance_jfk']=test.apply(lambda row:distance(row['pickup_latitude'],row['pickup_longitude'],jfk[1],jfk[0]),axis=1)\n",
    "test['dropoff_distance_jfk']=test.apply(lambda row:distance(row['dropoff_latitude'],row['dropoff_longitude'],jfk[1],jfk[0]),axis=1)\n",
    "test['pickup_distance_ewr']=test.apply(lambda row:distance(row['pickup_latitude'],row['pickup_longitude'],ewr[1],ewr[0]),axis=1)\n",
    "test['dropoff_distance_ewr']=test.apply(lambda row:distance(row['dropoff_latitude'],row['dropoff_longitude'],ewr[1],ewr[0]),axis=1)\n",
    "test['pickup_distance_laguardia']=test.apply(lambda row:distance(row['pickup_latitude'],row['pickup_longitude'],lgr[1],lgr[0]),axis=1)\n",
    "test['dropoff_distance_laguardia']=test.apply(lambda row:distance(row['dropoff_latitude'],row['dropoff_longitude'],lgr[1],lgr[0]),axis=1)\n",
    "\n",
    "\n",
    "\n",
    "train['pickup_distance_jfk']=train.apply(lambda row:distance(row['pickup_latitude'],row['pickup_longitude'],jfk[1],jfk[0]),axis=1)\n",
    "train['dropoff_distance_jfk']=train.apply(lambda row:distance(row['dropoff_latitude'],row['dropoff_longitude'],jfk[1],jfk[0]),axis=1)\n",
    "train['pickup_distance_ewr']=train.apply(lambda row:distance(row['pickup_latitude'],row['pickup_longitude'],ewr[1],ewr[0]),axis=1)\n",
    "train['dropoff_distance_ewr']=train.apply(lambda row:distance(row['dropoff_latitude'],row['dropoff_longitude'],ewr[1],ewr[0]),axis=1)\n",
    "train['pickup_distance_laguardia']=train.apply(lambda row:distance(row['pickup_latitude'],row['pickup_longitude'],lgr[1],lgr[0]),axis=1)\n",
    "train['dropoff_distance_laguardia']=train.apply(lambda row:distance(row['dropoff_latitude'],row['dropoff_longitude'],lgr[1],lgr[0]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "manhattan=(-73.9664, 40.7909)\n",
    "queens=(-73.8317, 40.7038)\n",
    "brooklyn=(-73.9489, 40.6551)\n",
    "bronx=(-73.8568, 40.8572)\n",
    "staten_island=(-74.1540, 40.5725)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test['pickup_distance_manhattan']=test.apply(lambda row:distance(row['pickup_latitude'],row['pickup_longitude'],manhattan[1],manhattan[0]),axis=1)\n",
    "test['pickup_distance_queens']=test.apply(lambda row:distance(row['pickup_latitude'],row['pickup_longitude'],queens[1],queens[0]),axis=1)\n",
    "test['pickup_distance_brooklyn']=test.apply(lambda row:distance(row['pickup_latitude'],row['pickup_longitude'],brooklyn[1],brooklyn[0]),axis=1)\n",
    "test['pickup_distance_bronx']=test.apply(lambda row:distance(row['pickup_latitude'],row['pickup_longitude'],bronx[1],bronx[0]),axis=1)\n",
    "test['pickup_distance_statenisland']=test.apply(lambda row:distance(row['pickup_latitude'],row['pickup_longitude'],staten_island[1],staten_island[0]),axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test['dropoff_distance_manhattan']=test.apply(lambda row:distance(row['dropoff_latitude'],row['dropoff_longitude'],manhattan[1],manhattan[0]),axis=1)\n",
    "test['dropoff_distance_queens']=test.apply(lambda row:distance(row['dropoff_latitude'],row['dropoff_longitude'],queens[1],queens[0]),axis=1)\n",
    "test['dropoff_distance_brooklyn']=test.apply(lambda row:distance(row['dropoff_latitude'],row['dropoff_longitude'],brooklyn[1],brooklyn[0]),axis=1)\n",
    "test['dropoff_distance_bronx']=test.apply(lambda row:distance(row['dropoff_latitude'],row['dropoff_longitude'],bronx[1],bronx[0]),axis=1)\n",
    "test['dropoff_distance_statenisland']=test.apply(lambda row:distance(row['dropoff_latitude'],row['dropoff_longitude'],staten_island[1],staten_island[0]),axis=1)\n",
    "\n",
    "train['pickup_distance_manhattan']=train.apply(lambda row:distance(row['pickup_latitude'],row['pickup_longitude'],manhattan[1],manhattan[0]),axis=1)\n",
    "train['pickup_distance_queens']=train.apply(lambda row:distance(row['pickup_latitude'],row['pickup_longitude'],queens[1],queens[0]),axis=1)\n",
    "train['pickup_distance_brooklyn']=train.apply(lambda row:distance(row['pickup_latitude'],row['pickup_longitude'],brooklyn[1],brooklyn[0]),axis=1)\n",
    "train['pickup_distance_bronx']=train.apply(lambda row:distance(row['pickup_latitude'],row['pickup_longitude'],bronx[1],bronx[0]),axis=1)\n",
    "train['pickup_distance_statenisland']=train.apply(lambda row:distance(row['pickup_latitude'],row['pickup_longitude'],staten_island[1],staten_island[0]),axis=1)\n",
    "\n",
    "train['dropoff_distance_manhattan']=train.apply(lambda row:distance(row['dropoff_latitude'],row['dropoff_longitude'],manhattan[1],manhattan[0]),axis=1)\n",
    "train['dropoff_distance_queens']=train.apply(lambda row:distance(row['dropoff_latitude'],row['dropoff_longitude'],queens[1],queens[0]),axis=1)\n",
    "train['dropoff_distance_brooklyn']=train.apply(lambda row:distance(row['dropoff_latitude'],row['dropoff_longitude'],brooklyn[1],brooklyn[0]),axis=1)\n",
    "train['dropoff_distance_bronx']=train.apply(lambda row:distance(row['dropoff_latitude'],row['dropoff_longitude'],bronx[1],bronx[0]),axis=1)\n",
    "train['dropoff_distance_statenisland']=train.apply(lambda row:distance(row['dropoff_latitude'],row['dropoff_longitude'],staten_island[1],staten_island[0]),axis=1)\n",
    "\n",
    "train.to_csv(\"train_cleaned.csv\")\n",
    "test.to_csv(\"test_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training Features (2348544, 29)\n",
      "Shape of Validation Features  (587136, 29)\n"
     ]
    }
   ],
   "source": [
    "###  We will apply same steps as above\n",
    "\n",
    "X_train, X_test, y_train, y_test=processDataForModelling(train,'fare_amount',drop_cols=['key','pickup_datetime'],is_train=True,split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Test Data (9914, 29)\n"
     ]
    }
   ],
   "source": [
    "test_data=processDataForModelling(test,'fare_amount',drop_cols=['key','pickup_datetime'],is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=lgb.Dataset(X_train,label=y_train)\n",
    "param = {'num_leaves':31, 'num_trees':1000, 'objective':'regression'}\n",
    "param['metric'] = 'l2_root'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\tcv_agg's rmse: 4.93857 + 0.0361967\n",
      "[40]\tcv_agg's rmse: 4.35581 + 0.0493811\n",
      "[60]\tcv_agg's rmse: 4.15574 + 0.0491483\n",
      "[80]\tcv_agg's rmse: 4.04744 + 0.0465886\n",
      "[100]\tcv_agg's rmse: 3.98069 + 0.0439523\n",
      "[120]\tcv_agg's rmse: 3.93519 + 0.0456173\n",
      "[140]\tcv_agg's rmse: 3.90411 + 0.0445432\n",
      "[160]\tcv_agg's rmse: 3.87923 + 0.0431755\n",
      "[180]\tcv_agg's rmse: 3.86033 + 0.0439698\n",
      "[200]\tcv_agg's rmse: 3.84439 + 0.0474349\n",
      "[220]\tcv_agg's rmse: 3.83107 + 0.0493404\n",
      "[240]\tcv_agg's rmse: 3.81917 + 0.0485925\n",
      "[260]\tcv_agg's rmse: 3.80654 + 0.048883\n",
      "[280]\tcv_agg's rmse: 3.79663 + 0.0477744\n",
      "[300]\tcv_agg's rmse: 3.78913 + 0.0483084\n",
      "[320]\tcv_agg's rmse: 3.78244 + 0.0486047\n",
      "[340]\tcv_agg's rmse: 3.7774 + 0.0484563\n",
      "[360]\tcv_agg's rmse: 3.77096 + 0.048162\n",
      "[380]\tcv_agg's rmse: 3.7657 + 0.0479438\n",
      "[400]\tcv_agg's rmse: 3.7622 + 0.0481963\n",
      "[420]\tcv_agg's rmse: 3.75704 + 0.0471727\n",
      "[440]\tcv_agg's rmse: 3.75243 + 0.0469251\n",
      "[460]\tcv_agg's rmse: 3.74771 + 0.0465064\n",
      "[480]\tcv_agg's rmse: 3.74503 + 0.0465838\n",
      "[500]\tcv_agg's rmse: 3.74074 + 0.0456097\n",
      "[520]\tcv_agg's rmse: 3.73676 + 0.0452711\n",
      "[540]\tcv_agg's rmse: 3.73333 + 0.0452988\n",
      "[560]\tcv_agg's rmse: 3.73003 + 0.04519\n",
      "[580]\tcv_agg's rmse: 3.72711 + 0.0450102\n",
      "[600]\tcv_agg's rmse: 3.72356 + 0.0461059\n",
      "[620]\tcv_agg's rmse: 3.72175 + 0.0459564\n",
      "[640]\tcv_agg's rmse: 3.71915 + 0.0461857\n",
      "[660]\tcv_agg's rmse: 3.71653 + 0.0459648\n",
      "[680]\tcv_agg's rmse: 3.71494 + 0.0463317\n",
      "[700]\tcv_agg's rmse: 3.7131 + 0.0462276\n",
      "[720]\tcv_agg's rmse: 3.7113 + 0.0459236\n",
      "[740]\tcv_agg's rmse: 3.70968 + 0.0461968\n",
      "[760]\tcv_agg's rmse: 3.70839 + 0.0459021\n",
      "[780]\tcv_agg's rmse: 3.70673 + 0.0462913\n",
      "[800]\tcv_agg's rmse: 3.70514 + 0.0465433\n",
      "[820]\tcv_agg's rmse: 3.70401 + 0.0466601\n",
      "[840]\tcv_agg's rmse: 3.70259 + 0.0465575\n",
      "[860]\tcv_agg's rmse: 3.70166 + 0.0465357\n",
      "[880]\tcv_agg's rmse: 3.70062 + 0.046868\n",
      "[900]\tcv_agg's rmse: 3.69945 + 0.0468997\n",
      "[920]\tcv_agg's rmse: 3.69872 + 0.0466536\n",
      "[940]\tcv_agg's rmse: 3.69762 + 0.0465212\n",
      "[960]\tcv_agg's rmse: 3.69641 + 0.0466249\n",
      "[980]\tcv_agg's rmse: 3.69563 + 0.0469055\n",
      "[1000]\tcv_agg's rmse: 3.69465 + 0.047453\n"
     ]
    }
   ],
   "source": [
    "num_round=5000\n",
    "cv_results = lgb.cv(param, train_data, num_boost_round=num_round, nfold=5,verbose_eval=20, early_stopping_rounds=20,stratified=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best num_boost_round: 1000\n"
     ]
    }
   ],
   "source": [
    "print('Best num_boost_round:', len(cv_results['rmse-mean']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_bst=lgb.train(param,train_data,len(cv_results['rmse-mean']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Light GBM with Feature Engineering is  3.6642689727498543\n"
     ]
    }
   ],
   "source": [
    "lgb_pred = lgb_bst.predict(X_test)\n",
    "lgb_rmse=np.sqrt(mean_squared_error(lgb_pred, y_test))\n",
    "print(\"RMSE for Light GBM with Feature Engineering is \",lgb_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better than without features , we can tune this model. The RMSE has decreased from 3.8 to 3.6. Let us get the best model by tuning the parameters. But, before that let us check what are the important features in our model and whether some features are not helping our model. But we saw above that Random Forest does better than Light GBM.So let us use rf in Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {'num_leaves':31,'num_trees':5000,\n",
    "'objective':'regression',\n",
    "\"boosting\":\"rf\",\n",
    "\"bagging_freq\":1,\n",
    " \"bagging_fraction\":0.4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "ename": "LightGBMError",
     "evalue": "Check failed: config->feature_fraction < 1.0f && config->feature_fraction > 0.0f at c:\\bld\\lightgbm_1531588417232\\work\\compile\\src\\boosting\\rf.hpp, line 30 .\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-173-fdc2ed65b4b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcv_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlgb_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_round\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnfold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstratified\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mcv\u001b[1;34m(params, train_set, num_boost_round, folds, nfold, stratified, shuffle, metrics, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, fpreproc, verbose_eval, show_stdv, seed, callbacks)\u001b[0m\n\u001b[0;32m    420\u001b[0m     cvfolds = _make_n_folds(train_set, folds=folds, nfold=nfold,\n\u001b[0;32m    421\u001b[0m                             \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfpreproc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfpreproc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m                             stratified=stratified, shuffle=shuffle)\n\u001b[0m\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[1;31m# setup callbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36m_make_n_folds\u001b[1;34m(full_data, folds, nfold, params, seed, fpreproc, stratified, shuffle)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[0mtparam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m         \u001b[0mcvbooster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m         \u001b[0mcvbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_valid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'valid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcvbooster\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, train_set, model_file, silent)\u001b[0m\n\u001b[0;32m   1307\u001b[0m                 \u001b[0mtrain_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m                 \u001b[0mc_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m                 ctypes.byref(self.handle)))\n\u001b[0m\u001b[0;32m   1310\u001b[0m             \u001b[1;31m# save reference to data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_safe_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \"\"\"\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecode_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLGBM_GetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLightGBMError\u001b[0m: Check failed: config->feature_fraction < 1.0f && config->feature_fraction > 0.0f at c:\\bld\\lightgbm_1531588417232\\work\\compile\\src\\boosting\\rf.hpp, line 30 .\n"
     ]
    }
   ],
   "source": [
    "cv_results = lgb.cv(lgb_params, train_data, num_boost_round=num_round, nfold=5,verbose_eval=20, early_stopping_rounds=20,stratified=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
