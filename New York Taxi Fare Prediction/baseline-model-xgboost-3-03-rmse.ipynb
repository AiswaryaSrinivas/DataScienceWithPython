{"cells":[{"metadata":{"_uuid":"b30b81f6ddca88dea3780f2dccac93ec23148c1f"},"cell_type":"markdown","source":"# Introduction\n In this kernel, based on the features generated in https://www.kaggle.com/aiswaryaramachandran/eda-and-feature-engineering we will create a baseline model. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport warnings\nfrom datetime import datetime\nimport calendar\nfrom math import sin, cos, sqrt, atan2, radians,asin\n\nfrom datetime import timedelta\nimport datetime as dt\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_colwidth', -1)\nplt.style.use('fivethirtyeight')\nimport folium\nfrom sklearn.cluster import KMeans\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nimport operator\nimport pickle\nimport os","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"input_path='../input/eda-and-feature-engineering//'\ntrain=pd.read_csv(input_path+'train_cleaned.csv')\ntest=pd.read_csv(input_path+'test_cleaned.csv')\nprint(\"Shape of Training Data \",train.shape)\nprint(\"Shape of Testing Data \",test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70eebd7d72e4b2946cc98a16dc54458cd0f15e8c"},"cell_type":"markdown","source":"### Pre-Processing data\n\n1. Remove the unwanted columns like pickup_datetime, key, pickup_date, latitude and longitude rounded to 3 decimal places\n2. One hot encoding of categorical variables\n3. Divide training data into train and validation datasets - 80% training and 20% validation"},{"metadata":{"trusted":true,"_uuid":"d5bd4c15f784feec9f7bcd181b2dcd2872c10abe"},"cell_type":"code","source":"drop_columns=['key','pickup_datetime','pickup_date','pickup_latitude_round3','pickup_longitude_round3','dropoff_latitude_round3','dropoff_longitude_round3']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"164b780dc1940aca96551538c525ccdade6e0a87"},"cell_type":"code","source":"train_1=train.drop(drop_columns,axis=1)\ntest_1=test.drop(drop_columns,axis=1)\nprint(\"Shape of Training Data after dropping columns\",train_1.shape)\nprint(\"Shape of Testing Data after dropping columns\",test_1.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3edfd231e356fad8c6b567f0cf4a46f1fd95ff9c"},"cell_type":"code","source":"train_1=pd.get_dummies(train_1)\ntest_1=pd.get_dummies(test_1)\n\nprint(\"Shape of Training Data after One Hot Encoding\",train_1.shape)\nprint(\"Shape of Testing Data after One Hot Encoding\",test_1.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f6a1a3c62bd5deba11406536ea57f4dd91710c5e"},"cell_type":"code","source":"X=train_1.drop(['fare_amount'],axis=1)\ny=train_1['fare_amount']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89be4c11f8a4f1775b39340fc8c0b89f4b1c6528"},"cell_type":"code","source":"#split data into train and validation data\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\nprint(\"Number of records in training data \",X_train.shape[0])\nprint(\"Number of records in validation data \",X_test.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d67358ee0b8b0c467edfd5415f66464202d8df9"},"cell_type":"markdown","source":"### Model Building\n#### 1. Linear Regression"},{"metadata":{"trusted":true,"_uuid":"79f5ac013809d2a7945cb01dd902bdae52746648"},"cell_type":"code","source":"lm = LinearRegression()\nlm.fit(X_train,y_train)\ny_pred=lm.predict(X_test)\nlm_rmse=np.sqrt(mean_squared_error(y_pred, y_test))\nprint(\"RMSE for Linear Regression is \",lm_rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0fb90d5b13b633b8d615f26c4753509aac034a49"},"cell_type":"code","source":"linear_reg_pred=lm.predict(test_1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da2915c03aae4e564038c28faee164ccaea69b7a"},"cell_type":"code","source":"submissions=pd.read_csv('../input/new-york-city-taxi-fare-prediction/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"29a02cf313a7d72daaba929d4bf7682e0ec4571e"},"cell_type":"code","source":"submissions['fare_amount']=linear_reg_pred\nsubmissions.to_csv(\"LinearRegression_Baseline.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f9a5942a80507ae233e9f40e80ee3c5daf955fc"},"cell_type":"markdown","source":"#### 2. XGBOOST"},{"metadata":{"trusted":true,"_uuid":"4c0c5f7d887b37190dcef9802984556747bc7d31"},"cell_type":"code","source":"def XGBoost(X_train,X_test,y_train,y_test,num_rounds=300):\n    dtrain = xgb.DMatrix(X_train,label=y_train)\n    dtest = xgb.DMatrix(X_test,label=y_test)\n\n    return xgb.train(params={'objective':'reg:linear','eval_metric':'rmse'}\n                    ,dtrain=dtrain,num_boost_round=num_rounds, \n                    early_stopping_rounds=20,evals=[(dtest,'test')],)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f9dbd926b28fbffb8808f73d77cf9f9c0bb5b02"},"cell_type":"code","source":"xgbm = XGBoost(X_train,X_test,y_train,y_test)\nxgbm_pred = xgbm.predict(xgb.DMatrix(test_1), ntree_limit = xgbm.best_ntree_limit)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27aae349b11258f0267abb29c11f5917efd87dab"},"cell_type":"code","source":"submissions['fare_amount']=xgbm_pred\nsubmissions.to_csv(\"XGboost_Baseline.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3203fe13bd5a943d9246a6f7ec71038754eb811"},"cell_type":"code","source":"importance=xgbm.get_score()\nimportance = sorted(importance.items(), key=operator.itemgetter(1))\ndf = pd.DataFrame(importance, columns=['feature', 'score'])\nplt.figure()\n\ndf.plot(kind='barh', x='feature', y='score', legend=False, figsize=(10, 25))\nplt.title(\"Feature Importance\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ff7a2babf2210fea283aa1e2071837a278b19c0"},"cell_type":"markdown","source":"The XGBoost scored 3.03760 on the leader board compared to rmse of 4.53265 for Linear Regression. From the importance graph above, we also observed that is_airport_pickup/dropoff features are not having more importance. But in the exploratory analysis we saw that There is a effect of airport on the price. \nTo take into consideration the same, we can add features like distance from the three airports"},{"metadata":{"trusted":true,"_uuid":"4b78d99681a2eaed8e7e6fa7a7393f69c5378a8d"},"cell_type":"code","source":"del train_1\ndel test_1\ndel X_train,X_test,y_train,y_test\nlgr=(-73.8733, 40.7746)\njfk=(-73.7900, 40.6437)\newr=(-74.1843, 40.6924)\n\n\ndef distance(lat1,lon1,lat2,lon2):\n    p = 0.017453292519943295 # Pi/180\n    a = 0.5 - np.cos((lat2 - lat1) * p)/2 + np.cos(lat1 * p) * np.cos(lat2 * p) * (1 - np.cos((lon2 - lon1) * p)) / 2\n    return 0.6213712 * 12742 * np.arcsin(np.sqrt(a))\n\ntest['pickup_distance_jfk']=test.apply(lambda row:distance(row['pickup_latitude'],row['pickup_longitude'],jfk[1],jfk[0]),axis=1)\ntest['dropoff_distance_jfk']=test.apply(lambda row:distance(row['dropoff_latitude'],row['dropoff_longitude'],jfk[1],jfk[0]),axis=1)\ntest['pickup_distance_ewr']=test.apply(lambda row:distance(row['pickup_latitude'],row['pickup_longitude'],ewr[1],ewr[0]),axis=1)\ntest['dropoff_distance_ewr']=test.apply(lambda row:distance(row['dropoff_latitude'],row['dropoff_longitude'],ewr[1],ewr[0]),axis=1)\ntest['pickup_distance_laguardia']=test.apply(lambda row:distance(row['pickup_latitude'],row['pickup_longitude'],lgr[1],lgr[0]),axis=1)\ntest['dropoff_distance_laguardia']=test.apply(lambda row:distance(row['dropoff_latitude'],row['dropoff_longitude'],lgr[1],lgr[0]),axis=1)\n\n\n\ntrain['pickup_distance_jfk']=train.apply(lambda row:distance(row['pickup_latitude'],row['pickup_longitude'],jfk[1],jfk[0]),axis=1)\ntrain['dropoff_distance_jfk']=train.apply(lambda row:distance(row['dropoff_latitude'],row['dropoff_longitude'],jfk[1],jfk[0]),axis=1)\ntrain['pickup_distance_ewr']=train.apply(lambda row:distance(row['pickup_latitude'],row['pickup_longitude'],ewr[1],ewr[0]),axis=1)\ntrain['dropoff_distance_ewr']=train.apply(lambda row:distance(row['dropoff_latitude'],row['dropoff_longitude'],ewr[1],ewr[0]),axis=1)\ntrain['pickup_distance_laguardia']=train.apply(lambda row:distance(row['pickup_latitude'],row['pickup_longitude'],lgr[1],lgr[0]),axis=1)\ntrain['dropoff_distance_laguardia']=train.apply(lambda row:distance(row['dropoff_latitude'],row['dropoff_longitude'],lgr[1],lgr[0]),axis=1)\n\n\ntrain_1=train.drop(drop_columns,axis=1)\ntest_1=test.drop(drop_columns,axis=1)\nprint(\"Shape of Training Data after dropping columns\",train_1.shape)\nprint(\"Shape of Testing Data after dropping columns\",test_1.shape)\n\n\ntrain_1=pd.get_dummies(train_1)\ntest_1=pd.get_dummies(test_1)\n\nprint(\"Shape of Training Data after One Hot Encoding\",train_1.shape)\nprint(\"Shape of Testing Data after One Hot Encoding\",test_1.shape)\n\nX=train_1.drop(['fare_amount'],axis=1)\ny=train_1['fare_amount']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\nprint(\"Number of records in training data \",X_train.shape[0])\nprint(\"Number of records in validation data \",X_test.shape[0])\n\nxgbm = XGBoost(X_train,X_test,y_train,y_test,num_rounds=1500)\nxgbm_pred = xgbm.predict(xgb.DMatrix(test_1), ntree_limit = xgbm.best_ntree_limit)\n\nsubmissions['fare_amount']=xgbm_pred\nsubmissions.to_csv(\"XGboost_WithDistancetoAirport.csv\",index=False)\n\nimportance=xgbm.get_score()\nimportance = sorted(importance.items(), key=operator.itemgetter(1))\ndf = pd.DataFrame(importance, columns=['feature', 'score'])\nplt.figure()\n\ndf.plot(kind='barh', x='feature', y='score', legend=False, figsize=(10, 25))\nplt.title(\"Feature Importance\")\n\ntrain.to_csv(\"train_cleaned.csv\",index=False)\ntest.to_csv(\"test_cleaned.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}